## 从微服务到云原生

本节引入了由 CNCF 定义的“云原生”概念，并强调了“不可变基础设施”是与微服务同等重要的核心原则。这个概念源于 Martin Fowler 的“凤凰服务器”思想，其核心价值在于将分布式架构的复杂性从应用层面剥离，隐藏到基础设施中，从而让复杂的分布式架构能够普及。

章节将以容器、编排系统（如 Kubernetes）和服务网格的发展为主线，阐述这些技术如何模糊软件与硬件的界限，在基础设施和通信层面解决原本需要由程序员在代码中处理的分布式难题，最终实现从微服务向云原生的演进。

### 虚拟化容器

本节首先定义了软件运行所需的兼容性（ISA、ABI、环境），并以此为基础，将虚拟化技术划分为五个层次：指令集虚拟化、硬件抽象层虚拟化、操作系统层虚拟化（即容器）、运行库虚拟化和语言层虚拟化。它明确了容器是一种操作系统层虚拟化，通过牺牲部分隔离性换取了比传统虚拟机更高的效率和更低的资源消耗。

### 容器的崛起

本节追溯了容器技术的发展史，这是一条从隔离到封装的演进之路：

- **隔离文件 (`chroot`)**: 最初的隔离思想，仅限于文件系统。
- **隔离访问 (`namespaces`)**: 内核提供的关键隔离机制，实现了对进程、用户、网络等多种资源的访问隔离，让进程仿佛独占系统。
- **隔离资源 (`cgroups`)**: 解决了资源分配和限制的问题，使得进程组可以拥有独立的资源配额。
- **封装系统 (`LXC`)**: 整合了 `namespaces` 和 `cgroups`，提供了第一个原生的系统级虚拟化容器方案，但其理念局限于“封装系统”。
- **封装应用 (`Docker`)**: 提出了“以应用为中心”的革命性理念，通过分层镜像、Dockerfile 等设计，极大地简化了应用的打包、分发和部署，并催生了庞大的生态。Docker 的发展也推动了 OCI 等标准的建立。
- **封装集群 (`Kubernetes`)**: 将容器编排提升到新的高度，虚拟化了整个集群环境。它通过引入 CRI（容器运行时接口）等标准，逐步与特定的容器引擎（如 Docker）解耦，成为了云原生时代的“操作系统”。

### 以容器构建系统

本节探讨了 Kubernetes 构建系统的两大核心设计理念：

- **隔离与协作 (Pod)**: 为了解决多容器协作问题，Kubernetes 引入了 `Pod` 的概念。Pod 作为调度的原子单位，其内部的容器共享网络、IPC 等名称空间，实现了“超亲密”协作，完美对应了操作系统的“进程组”概念。
- **韧性与弹性 (Controller)**: 采用了“控制回路”（Control Loop）的设计模式。用户通过声明式 API 定义资源的“期望状态”，而各种控制器（如 `ReplicaSet Controller`, `Deployment Controller`, `HPA Controller`）则不断地将“实际状态”调整至与期望一致。这种模式优雅地实现了故障自愈、滚动更新、自动扩缩容等能力，赋予了系统强大的韧性和弹性。

### 以应用为中心的封装

针对 Kubernetes 本身的复杂性，本节探讨了更高层次的应用封装方案，旨在分离不同角色（开发、运维、平台）的关注点：

- **Kustomize**: 通过 Base 和 Overlay 的方式，对 YAML 配置进行模板化管理，解决了不同环境下配置的差异化问题。
- **Helm**: 模仿 Linux 的包管理器，通过 Chart 格式封装应用，提供了应用安装、升级、依赖管理等全生命周期能力。
- **Operator**: 通过自定义资源（CRD）和自定义控制器，将应用的运维知识（特别是针对有状态应用）编码为自动化能力，实现了“高级指令”式的应用管理。
- **开放应用模型 (OAM)**: 提出了一个标准的、与平台无关的应用定义模型，通过 Component、Trait 等概念，彻底分离了开发、运维和平台的关注点，是目前最具前瞻性的探索方向之一。

---

# 第九部分：不可变基础设施 - 持久化存储

## 1. 容器存储的核心矛盾

- **容器的无状态性**: 容器镜像是不可变的，任何在容器运行时对文件系统的修改都基于“写入时复制”（Copy-on-Write）策略，存储在临时的、隔离的区域。一旦容器终止，这些改动就会丢失。
- **应用的有状态性**: 现实中的应用（如数据库、日志系统）需要持久化地保存数据，数据生命周期必须独立于容器的生命周期。

为了解决这个矛盾，容器必须引入外部的持久化存储方案。

## 2. Kubernetes 存储体系的演进

Kubernetes 围绕 `Volume` 概念构建了一套复杂的、分层的存储体系，以解耦应用（Pod）与具体的存储实现。

### 阶段一：静态存储分配 (Static Provisioning)

这是早期的存储管理方式，由管理员和用户协作完成。

1.  **`Volume` vs. `PersistentVolume` (PV)**:

    - **普通 `Volume`**: 生命周期与 `Pod` 绑定，用于在 `Pod` 内的多个容器间共享数据，但不是为了持久化。
    - **`PersistentVolume` (PV)**: 是由**管理员**预先准备好的、具体的网络存储资源（如一块 NFS、Ceph 或云厂商的磁盘）。它的生命周期独立于任何 `Pod`。

2.  **`PersistentVolumeClaim` (PVC)**:

    - 是**用户**（开发者）对自己应用所需存储能力的**声明**。PVC 中定义了需要多大空间、需要何种访问模式（如 `ReadWriteOnce`, `ReadOnlyMany`, `ReadWriteMany`）等。

3.  **绑定过程 (Binding)**:
    - 用户创建 `Pod` 和 `PVC`。
    - Kubernetes 控制平面会寻找一个能满足 `PVC` 声明条件的、处于可用状态的 `PV`。
    - 如果找到，就将 `PV` 和 `PVC` 进行一对一绑定。`Pod` 就可以通过挂载 `PVC` 来使用这块存储。
    - 如果找不到，`Pod` 将保持 `Pending` 状态，直到有合适的 `PV` 出现。

- **缺点**: 整个过程需要管理员手动介入，预先创建好足够多的 `PV`。在需要动态扩缩容的大规模集群中，这种方式效率低下，难以自动化。

### 阶段二：动态存储分配 (Dynamic Provisioning)

为了解决静态分配的局限性，Kubernetes 引入了 `StorageClass`，实现了存储的按需自动供给。

1.  **`StorageClass`**:

    - 由**管理员**定义，它描述了存储的“类型”。比如，可以定义一个名为 `fast-ssd` 的 `StorageClass` 指向云服务商的高性能 SSD 存储，再定义一个 `slow-hdd` 指向廉价的磁盘存储。
    - `StorageClass` 的核心是指定一个 **`Provisioner` (资源分配器)**，这个 `Provisioner` 知道如何与后端的存储系统（如 AWS EBS, GCE PD）交互来创建存储卷。

2.  **工作流程**:
    - 用户创建 `PVC` 时，不再是期望匹配一个已存在的 `PV`，而是在 `PVC` 中指定一个 `StorageClass` 的名称。
    - `StorageClass` 接管请求，命令其 `Provisioner` 根据 `PVC` 的要求（如 5GB 容量）去后端存储系统中**动态创建一个新的 `PV`**。
    - 新创建的 `PV` 会自动与该 `PVC` 绑定。
    - `Pod` 成功挂载 `PVC` 并启动。

- **优点**: 实现了完全的自动化。管理员只需维护好 `StorageClass`，无需关心具体的 `PV` 创建。用户也只需通过 `PVC` 声明意图，整个流程对用户透明，是目前推荐的主流方式。

## 3. 容器存储接口 (CSI) 与生态

为了将存储驱动的实现从 Kubernetes 核心代码中解耦出来，社区制定了标准的容器存储接口（CSI）。

### Kubernetes 存储架构三大操作

Kubernetes 将一个存储卷的挂载过程抽象为三个核心动作：

1.  **Provision / Delete**: 准备或移除存储资源（由 **PV 控制器** 触发）。
2.  **Attach / Detach**: 将存储设备附加或分离到指定节点（由 **AD 控制器** 触发）。
3.  **Mount / Unmount**: 将设备挂载或卸载到 `Pod` 内的指定路径（由节点上的 **Volume 管理器 (kubelet)** 触发）。

### 从 `FlexVolume` 到 `CSI`

- **`FlexVolume`**: Kubernetes 早期的私有存储扩展机制，实现简单（就是一个可执行文件），但功能有限（不支持 Provision），部署繁琐，现已冻结。
- **`CSI` (Container Storage Interface)**: 现行的标准接口，是一个开放的、基于 gRPC 的规范。存储厂商只需按照 CSI 规范实现 `Identity`、`Controller` 和 `Node` 三个服务，就能以插件的形式接入任何支持 CSI 的容器编排系统。
  - **CSI Controller**: 通常以 `StatefulSet` 部署，负责 `Provision`、`Attach` 等全局操作。
  - **CSI Node**: 以 `DaemonSet` 部署在每个节点上，负责 `Mount` 等节点级别的操作。

Kubernetes 正在将所有内置的（In-Tree）存储驱动迁移到外部的（Out-of-Tree）CSI 插件，以实现更好的解耦和扩展性。

## 4. 存储类型选型

后端存储系统主要分为三类，选择哪种取决于应用场景。

- **块存储 (Block Storage)**:

  - **特点**: 以固定大小的“块”为单位进行读写，性能最高，延迟最低。具有排他性，通常只能被单个节点以读写模式（RWO）挂载。
  - **例子**: 物理硬盘、AWS EBS。
  - **适用场景**: 需要极致 I/O 性能和低延迟的应用，如大型关系型数据库（Oracle）。

- **文件存储 (File Storage)**:

  - **特点**: 提供了文件、目录、权限等高级抽象，基于 POSIX 接口访问。支持多节点并发读写（RWX），易于共享。
  - **例子**: NFS、AWS EFS。
  - **适用场景**: 大多数通用应用的首选，特别是需要多 `Pod` 共享数据的场景，如 Web 内容、开发环境等。

- **对象存储 (Object Storage)**:
  - **特点**: 将数据和元数据打包成“对象”，通过唯一的 ID 访问。天然分布式，容量巨大，扩展成本极低，但延迟相对较高。
  - **例子**: AWS S3。
  - **适用场景**: 备份、归档、静态资源（图片、视频）托管、大数据分析等。通常通过 HTTP API 访问，而非直接挂载为文件系统。

---

# 第十章：不可变基础设施：资源调度

本章深入探讨了 Kubernetes 的核心功能之一——调度。调度器负责为新创建的 Pod 在集群中寻找最合适的节点来运行，其背后是一套复杂的资源管理、服务质量保障和优先级决策机制。

## 1. 资源模型

- **资源分类**：调度主要关注物理资源，特别是 CPU 和内存。
  - **可压缩资源 (CPU)**：资源不足时，Pod 的性能会下降（饥饿），但不会被终止。单位为 `Core` 或 `m` (Millicores)。
  - **不可压缩资源 (Memory)**：资源一旦耗尽或超出限制，Pod 会被系统杀死（OOM）。单位为 `Bytes`。
- **资源请求与限制 (`requests` vs `limits`)**：
  - `requests`：Pod 期望获得的资源量，是**调度器进行节点选择的依据**。
  - `limits`：Pod 允许使用的资源上限，是 **cgroups 强制执行的约束**。
  - 这种分离设计旨在提高集群的资源利用率，允许一定程度的资源超卖（Overcommit）。

## 2. 服务质量 (QoS) 与优先级

- **服务质量等级 (QoS Class)**：Kubernetes 根据 `requests` 和 `limits` 的设置，自动为 Pod 划分 QoS 等级，这是节点资源紧张时决定驱逐顺序的主要依据。

  - **`Guaranteed`**：最高等级。Pod 中所有容器都必须同时设置 `requests` 和 `limits`，且两者的值完全相等。这类 Pod 最后被驱逐。
  - **`Burstable`**：中等等级。Pod 中至少有一个容器的 `requests` 值小于 `limits` 值。
  - **`BestEffort`**：最低等级。Pod 中所有容器都没有设置 `requests` 和 `limits`。这类 Pod 最先被驱逐。

- **优先级 (Priority) 与抢占 (Preemption)**：
  - **`PriorityClass`**：一种集群级别的资源，用于显式定义 Pod 的重要性。
  - **调度影响**：高优先级的 Pod 会被优先调度。
  - **抢占机制**：当一个高优先级的 Pod 因资源不足而无法调度时，调度器会**驱逐**（杀死）一个或多个运行在节点上的、优先级更低的 Pod，为高优先级 Pod 腾出空间。

## 3. 驱逐机制 (Eviction)

- **触发**：由每个节点上的 `kubelet` 进程执行。当节点上的不可压缩资源（如内存、磁盘空间）低于预设的阈值时，驱逐机制被激活。
- **软驱逐 (Soft Eviction)**：当资源使用达到一个较低的“软”阈值时，会启动一个宽限期（Grace Period）。如果资源在宽限期内没有恢复，`kubelet` 会优雅地终止 Pod。
- **硬驱逐 (Hard Eviction)**：当资源使用达到一个更高的“硬”阈值时，`kubelet` 会立即强制杀死 Pod，以保证节点稳定。
- **驱逐后处理**：被驱逐的 Pod 通常由其控制器（如 ReplicaSet）重建。为避免新 Pod 被调度回同一个压力大的节点，Kubernetes 设有相应机制进行规避。

## 4. 默认调度器工作原理

Kubernetes 调度器采用**共享状态的双循环机制**，以实现高效调度。

- **两大循环**：

  - **Informer Loop**：监控 `etcd` 中 Pod 和 Node 等资源的变化，实时更新**调度队列**和**调度缓存**。
  - **Scheduler Loop**：不断从调度队列中取出待调度的 Pod，执行核心调度算法。

- **核心调度算法**：
  1.  **预选阶段 (Predicate / Filtering)**：
      - **目的**：过滤掉不满足 Pod 运行条件的节点。
      - **依据**：完全基于**调度缓存**中的数据，速度快。
      - **过滤策略**：包括资源是否足够（CPU、内存）、端口是否冲突、Volume 是否匹配、节点亲和性/反亲和性、污点与容忍度等。
  2.  **优选阶段 (Priority / Scoring)**：
      - **目的**：从通过预选的节点中，选出“最合适”的一个。
      - **方式**：对每个候选节点进行打分（0-10 分）。
      - **评分规则**：
        - `LeastRequestedPriority`：倾向于选择空闲资源（CPU 和内存）最多的节点。
        - `BalancedResourceAllocation`：倾向于选择资源使用率更均衡的节点。
        - 其他规则如 `ImageLocalityPriority`（优先选择已存在所需镜像的节点）等。
  3.  **绑定阶段 (Binding)**：
      - 选出得分最高的节点后，调度器并不会等待 Pod 创建完成。
      - 它采用**乐观绑定**策略：先在调度缓存中更新 Pod 的 `nodeName` 字段，然后异步地更新 `etcd`。
      - 目标节点上的 `kubelet` 监听到 `nodeName` 变化后，会接手后续的 Pod 创建流程。

---

# 第十一章：不可变基础设施：服务网格

本章系统性地介绍了服务网格（Service Mesh）的理念、演进、核心技术原理及其生态系统。服务网格是专用于处理服务间通信的基础设施层，它弥补了容器编排系统在微服务治理上的不足，将通信控制的粒度从容器级别细化到了单个服务。

## 1. 透明通信的演进之路

服务网格的出现并非一蹴而就，而是分布式系统通信方式演进的必然结果：

1.  **阶段一：业务与通信逻辑耦合**：开发者在业务代码中直接处理重试、熔断等分布式通信问题。专业性要求高，代码复用性差。
2.  **阶段二：抽取公共组件库**：将通信逻辑封装到公共库中（如 Spring Cloud、Finagle），实现了代码复用，但与特定编程语言和框架绑定，学习成本高。
3.  **阶段三：独立网络代理**：将通信能力移至进程外的独立代理，应用需主动访问代理。此模式演化出了微服务网关和边车代理两条路线。
4.  **阶段四：边车代理（Sidecar Proxy）**：将代理以边车容器的形式注入到应用 Pod 中，通过流量劫持，对应用透明且强制地接管所有进出流量。
5.  **阶段五：服务网格**：将所有边车代理统一管理起来，形成**数据平面**和**控制平面**，实现了通用、透明、可控的可靠通信。

## 2. 数据平面 (Data Plane)

数据平面由一系列的边车代理（如 Envoy）组成，负责直接处理和转发服务间的通信流量。

- **代理注入**：
  - **手动注入**：通过 `istioctl kube-inject` 等工具修改 Pod 的 YAML 文件，手动添加边车容器。
  - **自动注入**：利用 Kubernetes 的**动态准入控制**（`MutatingAdmissionWebhook`），在创建 Pod 时自动注入边车容器。这是 Istio 的推荐方式。
- **流量劫持**：
  - **iptables**：最经典的方式。通过在 Pod 的网络命名空间中设置 `iptables` 规则，将所有进出流量重定向到边车代理的特定端口。此方式通用但有一定性能损耗。
  - **eBPF**：一种更高效的内核技术，可在 Socket 层面直接转发流量，减少网络协议栈的穿越，性能更优。
  - **CNI 插件**：由服务网格提供自己的 CNI 网络插件，从网络底层接管流量，完全绕开 `iptables`。
- **可靠通信 (xDS 协议)**：
  - 数据平面的行为由控制平面通过 **xDS API** 动态配置。xDS (Discovery Service) 是一系列发现服务的统称。
  - **核心资源**：`Listener` (监听器)、`Route` (路由)、`Cluster` (上游服务集群)、`Endpoint` (集群实例)。
  - Envoy 等代理通过订阅 xDS 服务，动态获取路由规则、服务发现信息、负载均衡策略等，从而实现智能的流量治理。

## 3. 控制平面 (Control Plane)

控制平面（如 Istio）是服务网格的大脑，它不直接参与服务间的数据转发，而是负责管理和配置所有的边车代理。

- **Istio 架构演进**：从 1.5 版本开始，Istio 将原先的多个微服务组件（Pilot, Galley, Citadel, Mixer）合并为一个单体的 `istiod` 进程，大大简化了部署和维护。
- **核心功能**：
  - **策略分发**：作为 xDS 服务端，向数据平面下发所有流量治理和安全策略。
  - **流量控制**：通过 `VirtualService`、`DestinationRule` 等 CRD 资源，实现精细的请求路由（如金丝雀发布）、流量治理（熔断、超时、重试）和故障注入等高级功能。
  - **通信安全**：自动实现服务间的 mTLS 加密通信，管理证书、认证和授权策略。
  - **可观测性**：自动为所有网格内的流量生成详细的**日志**、**分布式追踪**和**监控指标**，无需修改应用代码。

## 4. 服务网格生态

服务网格领域仍在快速发展，并逐渐形成行业标准。

- **标准化**：
  - **SMI (Service Mesh Interface)**：由微软等厂商推动的、面向**控制平面**的标准化 API，旨在让上层应用能在不同服务网格产品间移植。
  - **UDPA (Universal Data Plane API)**：基于 Envoy xDS 演进而来的、面向**数据平面**的标准化 API，旨在统一代理与控制平面的交互方式。
- **主流产品**：
  - **数据平面**：`Envoy` (市场领导者), `Linkerd2-proxy` (Rust 开发，性能优异), `MOSN` (蚂蚁金服开源) 等。
  - **控制平面**：`Istio` (功能最强，生态最广), `Linkerd 2` (以轻量、简单著称), `Consul Connect` (集成 HashiCorp 生态), `Open Service Mesh (OSM)` (微软开源，SMI 的参考实现) 等。

服务网格通过将网络通信的复杂性从应用中剥离到基础设施层，极大地降低了微服务开发的门槛，是云原生技术栈中至关重要的一环。

---

# 第八部分：不可变基础设施 - 容器间网络

## 1. Linux 网络虚拟化基础

容器间的网络通信是构建在 Linux 网络虚拟化技术之上的。理解其底层原理至关重要。

### 核心概念：Linux 网络协议栈

- **分层模型**: Linux 网络通信遵循 OSI 或 TCP/IP 模型，数据在协议栈中自上而下（发送）或自下而上（接收）地进行逐层封装和解封装。
- **内核空间**: 几乎整个协议栈（应用层以下）都在内核空间，这是为了安全隔离，但也带来了用户态/内核态切换的性能开销。
- **数据流**:
  1.  **Socket**: 应用层通过 Socket API 与内核协议栈交互。
  2.  **TCP/UDP**: 传输层将数据封装成段（Segment），加入端口号、序列号等信息。
  3.  **IP**: 网络层将数据包封装成 IP 包，加入路由地址等信息。
  4.  **Device/Driver**: 网络访问层将 IP 包封装成以太帧（Frame），通过物理或虚拟网卡发送出去。

### 干预网络通信：Netfilter 与 iptables

- **Netfilter**: Linux 内核提供的一套钩子（Hooks）框架，允许代码在协议栈的关键位置（如 `PREROUTING`, `INPUT`, `FORWARD`, `OUTPUT`, `POSTROUTING`）干预数据包的处理。这是实现防火墙、NAT、流量控制等功能的基础。
- **iptables**: 一个用户态工具，它使用 Netfilter 钩子来过滤和修改 IP 包。它通过“表”（Tables）和“链”（Chains）来组织规则。
  - **五张表 (Tables)**: `raw`, `mangle`, `nat`, `filter`, `security`，各有不同用途且有优先级。
  - **五条链 (Chains)**: 对应 Netfilter 的五个钩子，规则在这些链上被触发。
- **在容器中的作用**: Kubernetes 的 `kube-proxy` 就严重依赖 `iptables`（或 IPVS，也基于 Netfilter）来实现 `Service` 的负载均衡和 `ClusterIP` 到 `Pod` 的流量转发。

### 虚拟网络设备

- **`tun/tap`**:
  - `tap`: 模拟二层以太网设备，处理以太帧。
  - `tun`: 模拟三层网络设备，处理 IP 包。
  - 核心作用是**将协议栈中的数据包转发给用户态应用程序处理**，常用于 VPN、数据压缩、加密等场景。
- **`veth`**:

  - 一种**成对出现的虚拟设备**，像一根虚拟网线，一端插入容器的网络名称空间，另一端留在宿主机上（通常会接入虚拟交换机）。
  - 这是容器网络中最主流的连接方案，为容器提供了独立的网络协议栈，同时又能与外部网络通信。

- **网络名称空间 (Network Namespace)**:

  - 为每个容器提供了隔离的、完整的网络协议栈，包括独立的网络设备、路由表、IP 地址、iptables 规则等。
  - `veth` 设备对就是为了打通不同网络名称空间之间的通信。

- **虚拟交换机 (Bridge)**:
  - Linux Bridge 是一个工作在二层的虚拟设备，功能类似于物理交换机。
  - 宿主机上的 `veth` 端点通常会连接到一个 Bridge 上（如 Docker 的 `docker0`），所有连接到该 Bridge 的容器就可以相互通信。
  - 当需要跨主机通信时，数据包会通过宿主机的物理网卡，经由外部网络路由到目标主机。

## 2. 容器网络接口 (CNI) 与生态

为了将网络功能从容器运行时中解耦，社区发展出了标准的容器网络接口。

### CNM vs. CNI

- **CNM (Container Network Model)**: 由 Docker 提出，与 `libnetwork` 库一同发布。设计上与 Docker 深度绑定，虽然吸引了部分厂商，但因其封闭性未被 Kubernetes 采纳。
- **CNI (Container Networking Interface)**: 最初由 CoreOS 为其 `rkt` 容器引擎提出，后与 Google 合作，成为 Kubernetes 支持的标准。CNI 的设计更轻量、更开放，最终赢得了社区，成为事实标准。

### CNI 插件核心功能

CNI 插件就是一个可执行文件，它负责两件核心事情：

1.  **网络管理**: 创建网络、将容器接入网络、删除网络等。
2.  **IP 地址管理 (IPAM)**: 为容器分配和回收全局唯一的 IP 地址。

### 主流 CNI 插件实现模式

- **Overlay 模式**:

  - **原理**: 在现有的物理网络（Underlay）之上构建一个虚拟的逻辑网络。它通过隧道技术（如 VXLAN、IPIP）将原始数据包进行二次封装，从而跨越底层网络的限制（如不同二层子网）。
  - **优点**: 灵活性高，不受底层网络拓扑限制，易于部署。
  - **缺点**: 封装/解封装带来性能开销，导致延迟增加、吞吐量下降。
  - **代表插件**: Flannel (VXLAN 模式), Calico (IPIP 模式)。

- **路由 (Routing) 模式**:

  - **原理**: 不进行隧道封装，而是通过修改主机上的路由表，让主机本身成为路由器，直接转发跨主机的容器数据包。
  - **优点**: 性能远高于 Overlay 模式，接近物理网络。
  - **缺点**: 强依赖底层网络环境。要求所有主机在同一二层子网内，或者中间的路由设备支持 BGP 等动态路由协议。
  - **代表插件**: Flannel (Host-Gateway 模式), Calico (BGP 模式)。

- **Underlay 模式**:
  - **原理**: 让容器直接使用底层物理网络的网络接口，与宿主机在网络中处于同等地位。
  - **优点**: 性能最好，能最大化利用硬件能力。
  - **缺点**: 灵活性差，部署复杂，强依赖底层硬件和网络环境。
  - **代表插件**: MACVLAN, SR-IOV。

### 如何选择 CNI 插件？

1.  **环境支持**: 首先要看物理/虚拟化环境支持哪种模式。云环境通常限制较多，Overlay 模式更普适。
2.  **性能 vs. 灵活性**:
    - 对网络性能要求极致的场景（如 NFV），优先考虑 Underlay 或路由模式。
    - 对于大多数 Web 应用，网络 I/O 通常不是瓶颈，Overlay 模式的灵活性和易用性带来的好处可能超过其性能损失。
3.  **功能需求**:
    - CNI 插件只负责网络连通性。如果需要网络策略（NetworkPolicy）等高级功能，需要选择支持这些功能的插件，如 Calico、Weave，而 Flannel 本身不支持。

---

> **Note**: `plugin.md` 和 `strategy.md` 文件当前为空，仅包含标题，因此本笔记主要基于 `README.md`, `linux-vnet.md`, 和 `cni.md` 的内容。
