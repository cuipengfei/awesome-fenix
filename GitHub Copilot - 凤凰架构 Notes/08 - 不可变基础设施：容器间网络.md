# 第八部分：不可变基础设施 - 容器间网络

## 1. Linux 网络虚拟化基础

容器间的网络通信是构建在 Linux 网络虚拟化技术之上的。理解其底层原理至关重要。

### 核心概念：Linux 网络协议栈

- **分层模型**: Linux 网络通信遵循 OSI 或 TCP/IP 模型，数据在协议栈中自上而下（发送）或自下而上（接收）地进行逐层封装和解封装。
- **内核空间**: 几乎整个协议栈（应用层以下）都在内核空间，这是为了安全隔离，但也带来了用户态/内核态切换的性能开销。
- **数据流**:
  1.  **Socket**: 应用层通过 Socket API 与内核协议栈交互。
  2.  **TCP/UDP**: 传输层将数据封装成段（Segment），加入端口号、序列号等信息。
  3.  **IP**: 网络层将数据包封装成 IP 包，加入路由地址等信息。
  4.  **Device/Driver**: 网络访问层将 IP 包封装成以太帧（Frame），通过物理或虚拟网卡发送出去。

### 干预网络通信：Netfilter 与 iptables

- **Netfilter**: Linux 内核提供的一套钩子（Hooks）框架，允许代码在协议栈的关键位置（如 `PREROUTING`, `INPUT`, `FORWARD`, `OUTPUT`, `POSTROUTING`）干预数据包的处理。这是实现防火墙、NAT、流量控制等功能的基础。
- **iptables**: 一个用户态工具，它使用 Netfilter 钩子来过滤和修改 IP 包。它通过“表”（Tables）和“链”（Chains）来组织规则。
  - **五张表 (Tables)**: `raw`, `mangle`, `nat`, `filter`, `security`，各有不同用途且有优先级。
  - **五条链 (Chains)**: 对应 Netfilter 的五个钩子，规则在这些链上被触发。
- **在容器中的作用**: Kubernetes 的 `kube-proxy` 就严重依赖 `iptables`（或 IPVS，也基于 Netfilter）来实现 `Service` 的负载均衡和 `ClusterIP` 到 `Pod` 的流量转发。

### 虚拟网络设备

- **`tun/tap`**:
  - `tap`: 模拟二层以太网设备，处理以太帧。
  - `tun`: 模拟三层网络设备，处理 IP 包。
  - 核心作用是**将协议栈中的数据包转发给用户态应用程序处理**，常用于 VPN、数据压缩、加密等场景。
- **`veth`**:

  - 一种**成对出现的虚拟设备**，像一根虚拟网线，一端插入容器的网络名称空间，另一端留在宿主机上（通常会接入虚拟交换机）。
  - 这是容器网络中最主流的连接方案，为容器提供了独立的网络协议栈，同时又能与外部网络通信。

- **网络名称空间 (Network Namespace)**:

  - 为每个容器提供了隔离的、完整的网络协议栈，包括独立的网络设备、路由表、IP 地址、iptables 规则等。
  - `veth` 设备对就是为了打通不同网络名称空间之间的通信。

- **虚拟交换机 (Bridge)**:
  - Linux Bridge 是一个工作在二层的虚拟设备，功能类似于物理交换机。
  - 宿主机上的 `veth` 端点通常会连接到一个 Bridge 上（如 Docker 的 `docker0`），所有连接到该 Bridge 的容器就可以相互通信。
  - 当需要跨主机通信时，数据包会通过宿主机的物理网卡，经由外部网络路由到目标主机。

## 2. 容器网络接口 (CNI) 与生态

为了将网络功能从容器运行时中解耦，社区发展出了标准的容器网络接口。

### CNM vs. CNI

- **CNM (Container Network Model)**: 由 Docker 提出，与 `libnetwork` 库一同发布。设计上与 Docker 深度绑定，虽然吸引了部分厂商，但因其封闭性未被 Kubernetes 采纳。
- **CNI (Container Networking Interface)**: 最初由 CoreOS 为其 `rkt` 容器引擎提出，后与 Google 合作，成为 Kubernetes 支持的标准。CNI 的设计更轻量、更开放，最终赢得了社区，成为事实标准。

### CNI 插件核心功能

CNI 插件就是一个可执行文件，它负责两件核心事情：

1.  **网络管理**: 创建网络、将容器接入网络、删除网络等。
2.  **IP 地址管理 (IPAM)**: 为容器分配和回收全局唯一的 IP 地址。

### 主流 CNI 插件实现模式

- **Overlay 模式**:

  - **原理**: 在现有的物理网络（Underlay）之上构建一个虚拟的逻辑网络。它通过隧道技术（如 VXLAN、IPIP）将原始数据包进行二次封装，从而跨越底层网络的限制（如不同二层子网）。
  - **优点**: 灵活性高，不受底层网络拓扑限制，易于部署。
  - **缺点**: 封装/解封装带来性能开销，导致延迟增加、吞吐量下降。
  - **代表插件**: Flannel (VXLAN 模式), Calico (IPIP 模式)。

- **路由 (Routing) 模式**:

  - **原理**: 不进行隧道封装，而是通过修改主机上的路由表，让主机本身成为路由器，直接转发跨主机的容器数据包。
  - **优点**: 性能远高于 Overlay 模式，接近物理网络。
  - **缺点**: 强依赖底层网络环境。要求所有主机在同一二层子网内，或者中间的路由设备支持 BGP 等动态路由协议。
  - **代表插件**: Flannel (Host-Gateway 模式), Calico (BGP 模式)。

- **Underlay 模式**:
  - **原理**: 让容器直接使用底层物理网络的网络接口，与宿主机在网络中处于同等地位。
  - **优点**: 性能最好，能最大化利用硬件能力。
  - **缺点**: 灵活性差，部署复杂，强依赖底层硬件和网络环境。
  - **代表插件**: MACVLAN, SR-IOV。

### 如何选择 CNI 插件？

1.  **环境支持**: 首先要看物理/虚拟化环境支持哪种模式。云环境通常限制较多，Overlay 模式更普适。
2.  **性能 vs. 灵活性**:
    - 对网络性能要求极致的场景（如 NFV），优先考虑 Underlay 或路由模式。
    - 对于大多数 Web 应用，网络 I/O 通常不是瓶颈，Overlay 模式的灵活性和易用性带来的好处可能超过其性能损失。
3.  **功能需求**:
    - CNI 插件只负责网络连通性。如果需要网络策略（NetworkPolicy）等高级功能，需要选择支持这些功能的插件，如 Calico、Weave，而 Flannel 本身不支持。

---

> **Note**: `plugin.md` 和 `strategy.md` 文件当前为空，仅包含标题，因此本笔记主要基于 `README.md`, `linux-vnet.md`, 和 `cni.md` 的内容。
